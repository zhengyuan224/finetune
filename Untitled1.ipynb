{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ec68f2-eb71-4d0e-b0e2-ab9c4d3c239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/30/2024 17:04:24 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-07-30 17:04:24,203 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-07-30 17:04:24,203 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-07-30 17:04:24,203 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-07-30 17:04:24,203 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-07-30 17:04:24,204 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-07-30 17:04:24,204 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2533] 2024-07-30 17:04:24,455 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/30/2024 17:04:24 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/30/2024 17:04:24 - INFO - llamafactory.data.loader - Loading dataset data.json...\n",
      "Generating train split: 1421 examples [00:00, 65241.87 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 1421/1421 [00:00<00:00, 8168\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1421/1421 [00:02<00:00, 601.\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 104133, 105205, 26939, 24339, 16744, 9370, 110376, 20742, 3837, 102298, 87752, 103124, 110376, 48443, 16, 13, 279, 1464, 512, 198, 17, 13, 8251, 1464, 6236, 198, 18, 13, 34208, 1464, 822, 6128, 198, 19, 13, 916, 1464, 1729, 198, 20, 13, 17788, 1464, 326, 2886, 198, 21, 13, 19362, 1464, 348, 1777, 198, 22, 13, 11088, 1464, 99096, 198, 23, 13, 282, 3310, 1464, 4634, 263, 198, 24, 13, 18636, 1464, 29770, 15525, 198, 16, 15, 13, 17066, 1464, 939, 521, 2095, 198, 16, 16, 13, 1007, 1464, 409, 198, 16, 17, 13, 7002, 1464, 8146, 271, 100345, 99487, 110376, 20742, 3837, 105395, 87752, 105205, 109949, 12857, 24339, 16744, 28311, 50404, 33872, 220, 16, 28311, 105205, 109949, 330, 1782, 8251, 34208, 916, 279, 17788, 1, 10236, 123, 119, 102610, 12857, 24339, 16744, 20412, 28311, 32, 25, 273, 6236, 822, 1070, 1729, 1187, 326, 2886, 198, 33, 25, 273, 6236, 822, 6128, 1729, 512, 326, 2886, 198, 34, 25, 273, 822, 6128, 6236, 1729, 512, 326, 2886, 198, 35, 25, 273, 6236, 822, 6128, 1729, 512, 326, 2886, 151645, 198, 151644, 77091, 198, 35, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "有一个英文到法文的词汇表，包含以下对应词汇：\n",
      "\n",
      "1. the -> le\n",
      "2. cat -> chat\n",
      "3. jumps -> sauts\n",
      "4. over -> sur\n",
      "5. moon -> lune\n",
      "6. cow -> vache\n",
      "7. plays -> jouer\n",
      "8. fiddle -> violon\n",
      "9. egg -> bougre\n",
      "10. falls -> des chutes\n",
      "11. off -> de\n",
      "12. wall -> mur\n",
      "\n",
      "根据这个词汇表，翻译以下英文句子成法文：\n",
      "选择题 1：\n",
      "英文句子 \"the cat jumps over the moon\" 翻译成法文是：\n",
      "A:le chat saute sur la lune\n",
      "B:le chat sauts sur le lune\n",
      "C:le sauts chat sur le lune\n",
      "D:le chat sauts sur le lune<|im_end|>\n",
      "<|im_start|>assistant\n",
      "D<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 35, 151645]\n",
      "labels:\n",
      "D<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-30 17:04:29,070 >> loading configuration file /root/autodl-tmp/qwen/Qwen2-7B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-30 17:04:29,072 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/qwen/Qwen2-7B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3631] 2024-07-30 17:04:29,109 >> loading weights file /root/autodl-tmp/qwen/Qwen2-7B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1572] 2024-07-30 17:04:29,109 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1038] 2024-07-30 17:04:29,111 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.22s/it]\n",
      "[INFO|modeling_utils.py:4463] 2024-07-30 17:04:34,281 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4471] 2024-07-30 17:04:34,281 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/qwen/Qwen2-7B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:991] 2024-07-30 17:04:34,283 >> loading configuration file /root/autodl-tmp/qwen/Qwen2-7B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-07-30 17:04:34,284 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/30/2024 17:04:34 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/30/2024 17:04:34 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/30/2024 17:04:34 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/30/2024 17:04:34 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/30/2024 17:04:34 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,q_proj,down_proj,o_proj,gate_proj,up_proj,k_proj\n",
      "07/30/2024 17:04:35 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n",
      "Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:648] 2024-07-30 17:04:35,143 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2134] 2024-07-30 17:04:35,446 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-07-30 17:04:35,446 >>   Num examples = 1,421\n",
      "[INFO|trainer.py:2136] 2024-07-30 17:04:35,447 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2137] 2024-07-30 17:04:35,447 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2140] 2024-07-30 17:04:35,447 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2141] 2024-07-30 17:04:35,447 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2142] 2024-07-30 17:04:35,447 >>   Total optimization steps = 531\n",
      "[INFO|trainer.py:2143] 2024-07-30 17:04:35,451 >>   Number of trainable parameters = 20,185,088\n",
      "{'loss': 3.1008, 'grad_norm': 2.6312835216522217, 'learning_rate': 9.991251692769932e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3528, 'grad_norm': 2.9514567852020264, 'learning_rate': 9.965037384231488e-05, 'epoch': 0.11}\n",
      "{'loss': 0.4107, 'grad_norm': 4.291453838348389, 'learning_rate': 9.921448806714631e-05, 'epoch': 0.17}\n",
      "{'loss': 0.2932, 'grad_norm': 1.0915738344192505, 'learning_rate': 9.860638490726497e-05, 'epoch': 0.23}\n",
      "{'loss': 0.2814, 'grad_norm': 1.6873842477798462, 'learning_rate': 9.782819231197898e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3737, 'grad_norm': 3.5164332389831543, 'learning_rate': 9.68826334284514e-05, 'epoch': 0.34}\n",
      "{'loss': 0.4135, 'grad_norm': 2.6741907596588135, 'learning_rate': 9.577301707252912e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3132, 'grad_norm': 1.3377351760864258, 'learning_rate': 9.450322615012783e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3526, 'grad_norm': 3.0808756351470947, 'learning_rate': 9.30777040696903e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3373, 'grad_norm': 2.8275840282440186, 'learning_rate': 9.150143919326577e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2732, 'grad_norm': 1.6079058647155762, 'learning_rate': 8.977994738062017e-05, 'epoch': 0.62}\n",
      "{'loss': 0.283, 'grad_norm': 3.9051856994628906, 'learning_rate': 8.791925268746193e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2625, 'grad_norm': 1.6950855255126953, 'learning_rate': 8.592586628532588e-05, 'epoch': 0.73}\n",
      "{'loss': 0.373, 'grad_norm': 2.807286500930786, 'learning_rate': 8.380676367688172e-05, 'epoch': 0.79}\n",
      "{'loss': 0.283, 'grad_norm': 2.9944100379943848, 'learning_rate': 8.156936028639767e-05, 'epoch': 0.84}\n",
      "{'loss': 0.2039, 'grad_norm': 1.7761963605880737, 'learning_rate': 7.922148551077681e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3336, 'grad_norm': 1.7541172504425049, 'learning_rate': 7.677135532196905e-05, 'epoch': 0.96}\n",
      "{'loss': 0.2745, 'grad_norm': 1.8198041915893555, 'learning_rate': 7.422754351663252e-05, 'epoch': 1.01}\n",
      "{'loss': 0.1393, 'grad_norm': 1.512487530708313, 'learning_rate': 7.159895171365065e-05, 'epoch': 1.07}\n",
      "{'loss': 0.1218, 'grad_norm': 1.895662784576416, 'learning_rate': 6.889477820449342e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0862, 'grad_norm': 1.4140515327453613, 'learning_rate': 6.612448576542545e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1505, 'grad_norm': 7.818722248077393, 'learning_rate': 6.329776854419636e-05, 'epoch': 1.24}\n",
      "{'loss': 0.1183, 'grad_norm': 1.2619078159332275, 'learning_rate': 6.0424518137087674e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0456, 'grad_norm': 0.07455241680145264, 'learning_rate': 5.751478897502352e-05, 'epoch': 1.35}\n",
      "{'loss': 0.128, 'grad_norm': 7.441167831420898, 'learning_rate': 5.457876313987034e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0674, 'grad_norm': 0.345338374376297, 'learning_rate': 5.162671473404461e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0611, 'grad_norm': 0.19291095435619354, 'learning_rate': 4.866897392811126e-05, 'epoch': 1.52}\n",
      "{'loss': 0.1569, 'grad_norm': 5.681781768798828, 'learning_rate': 4.571589081218116e-05, 'epoch': 1.58}\n",
      "{'loss': 0.1812, 'grad_norm': 0.15583498775959015, 'learning_rate': 4.277779917760393e-05, 'epoch': 1.63}\n",
      "{'loss': 0.1023, 'grad_norm': 5.003534317016602, 'learning_rate': 3.986498035569532e-05, 'epoch': 1.69}\n",
      "{'loss': 0.105, 'grad_norm': 0.3680548071861267, 'learning_rate': 3.698762724003919e-05, 'epoch': 1.75}\n",
      "{'loss': 0.164, 'grad_norm': 2.512610673904419, 'learning_rate': 3.4155808618261563e-05, 'epoch': 1.8}\n",
      "{'loss': 0.074, 'grad_norm': 5.1395745277404785, 'learning_rate': 3.1379433938091696e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0695, 'grad_norm': 0.7117583751678467, 'learning_rate': 2.866821863100479e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0685, 'grad_norm': 0.5891168117523193, 'learning_rate': 2.603165011479012e-05, 'epoch': 1.97}\n",
      "{'loss': 0.1442, 'grad_norm': 0.12001562863588333, 'learning_rate': 2.347895459401288e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0066, 'grad_norm': 0.07115936279296875, 'learning_rate': 2.1019064774545282e-05, 'epoch': 2.08}\n",
      "{'loss': 0.035, 'grad_norm': 0.05888260155916214, 'learning_rate': 1.8660588605144484e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0429, 'grad_norm': 1.2461791038513184, 'learning_rate': 1.641177915546036e-05, 'epoch': 2.2}\n",
      "{'loss': 0.015, 'grad_norm': 0.11823304742574692, 'learning_rate': 1.428050573587999e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0206, 'grad_norm': 0.30996695160865784, 'learning_rate': 1.2274226360269687e-05, 'epoch': 2.31}\n",
      "{'loss': 0.027, 'grad_norm': 2.310643196105957, 'learning_rate': 1.0399961647976314e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0254, 'grad_norm': 0.0344107411801815, 'learning_rate': 8.664270256413332e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0167, 'grad_norm': 0.023524058982729912, 'learning_rate': 7.073225930200722e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0014, 'grad_norm': 0.018738726153969765, 'learning_rate': 5.6323962471714286e-06, 'epoch': 2.53}\n",
      "{'loss': 0.036, 'grad_norm': 6.282242774963379, 'learning_rate': 4.346823135618788e-06, 'epoch': 2.59}\n",
      "{'loss': 0.016, 'grad_norm': 0.016073083505034447, 'learning_rate': 3.2210052309614437e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0031, 'grad_norm': 0.03540824353694916, 'learning_rate': 2.258882133565404e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0153, 'grad_norm': 0.6680836081504822, 'learning_rate': 1.4638206228103413e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0042, 'grad_norm': 0.11670244485139847, 'learning_rate': 8.386028756414089e-07, 'epoch': 2.81}\n",
      " 94%|██████████████████████████████████████▌  | 500/531 [14:23<00:51,  1.67s/it][INFO|trainer.py:3503] 2024-07-30 17:18:59,285 >> Saving model checkpoint to model/checkpoint-500\n",
      "[INFO|configuration_utils.py:731] 2024-07-30 17:18:59,309 >> loading configuration file /root/autodl-tmp/qwen/Qwen2-7B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-30 17:18:59,309 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-07-30 17:18:59,430 >> tokenizer config file saved in model/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-07-30 17:18:59,430 >> Special tokens file saved in model/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.033, 'grad_norm': 1.1500566005706787, 'learning_rate': 3.854167308337708e-07, 'epoch': 2.87}\n",
      "{'loss': 0.0481, 'grad_norm': 0.7228586077690125, 'learning_rate': 1.0584803303831381e-07, 'epoch': 2.93}\n",
      "{'loss': 0.0776, 'grad_norm': 0.05705161392688751, 'learning_rate': 8.750833991155727e-10, 'epoch': 2.98}\n",
      "100%|█████████████████████████████████████████| 531/531 [15:16<00:00,  1.68s/it][INFO|trainer.py:3503] 2024-07-30 17:19:52,383 >> Saving model checkpoint to model/checkpoint-531\n",
      "[INFO|configuration_utils.py:731] 2024-07-30 17:19:52,405 >> loading configuration file /root/autodl-tmp/qwen/Qwen2-7B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-30 17:19:52,406 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-07-30 17:19:52,534 >> tokenizer config file saved in model/checkpoint-531/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-07-30 17:19:52,534 >> Special tokens file saved in model/checkpoint-531/special_tokens_map.json\n",
      "[INFO|trainer.py:2394] 2024-07-30 17:19:52,941 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 917.4897, 'train_samples_per_second': 4.646, 'train_steps_per_second': 0.579, 'train_loss': 0.20574002283258738, 'epoch': 2.99}\n",
      "100%|█████████████████████████████████████████| 531/531 [15:17<00:00,  1.73s/it]\n",
      "[INFO|trainer.py:3503] 2024-07-30 17:19:52,943 >> Saving model checkpoint to model\n",
      "[INFO|configuration_utils.py:731] 2024-07-30 17:19:52,965 >> loading configuration file /root/autodl-tmp/qwen/Qwen2-7B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-30 17:19:52,965 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-07-30 17:19:53,089 >> tokenizer config file saved in model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-07-30 17:19:53,089 >> Special tokens file saved in model/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9894\n",
      "  total_flos               = 32926676GF\n",
      "  train_loss               =     0.2057\n",
      "  train_runtime            = 0:15:17.48\n",
      "  train_samples_per_second =      4.646\n",
      "  train_steps_per_second   =      0.579\n",
      "Figure saved at: model/training_loss.png\n",
      "07/30/2024 17:19:53 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
      "07/30/2024 17:19:53 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:449] 2024-07-30 17:19:53,416 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train llama3_lora_sft.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05160b6e-836d-4cbf-b7f8-009b0fd07630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
